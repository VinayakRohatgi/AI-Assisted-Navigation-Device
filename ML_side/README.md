# AI-Assisted Navigation Device - ML System

ğŸ¯ **Complete navigation system for visually impaired users in library environments**

## ğŸš€ System Overview

The AI-Assisted Navigation Device provides intelligent navigation assistance through:

- **ğŸ” Real-time Object Detection**: YOLO-based detection of library furniture and equipment
- **ğŸ—ºï¸ Semantic Mapping**: Intelligent understanding of library environments and zones
- **ğŸ§  Scene Memory**: Temporal tracking of objects and environmental changes
- **ğŸš€ Advanced Pathfinding**: Multiple algorithms (A*, D*, RRT*) for optimal navigation
- **ğŸ’¬ Natural Language Processing**: LLM-powered navigation guidance and reasoning
- **ğŸ¯ Integrated Planning**: Complete pipeline from detection to actionable guidance

## âœ… System Status: **FULLY FUNCTIONAL**

### ğŸ† Sprint 1 Complete - All Core Components Implemented:

- **âœ… YOLO Object Detection** - 85.7% mAP@0.5, detecting 6 library object classes
- **âœ… LLM Integration** - OpenAI API + fallback reasoning for intelligent guidance  
- **âœ… Semantic Mapping** - Library zone classification and spatial understanding
- **âœ… Scene Memory System** - Object tracking and temporal awareness
- **âœ… A* Pathfinding** - Optimal path planning (30-50ms performance)
- **âœ… RRT* Pathfinding** - Complex environment navigation
- **âœ… Navigation Planner** - Integrated system with automatic algorithm selection
- **âœ… Comprehensive Testing** - Full test suite with 6/6 passing tests

## ğŸ—ï¸ Project Structure

```
ML_side/
â”œâ”€â”€ src/                           # Core system modules
â”‚   â”œâ”€â”€ llm_integration/           # LLM reasoning and navigation pipeline
â”‚   â”œâ”€â”€ semantic_mapping/          # Environment understanding and memory
â”‚   â””â”€â”€ pathfinding/              # Navigation algorithms (A*, D*, RRT*)
â”œâ”€â”€ models/object_detection/       # Trained YOLO model weights
â”œâ”€â”€ data/processed/               # Training and validation datasets
â”œâ”€â”€ config/                       # System configuration
â”œâ”€â”€ experiments/                  # Model training results
â”œâ”€â”€ notebooks/                    # Development and analysis notebooks
â”œâ”€â”€ demo.py                       # Live system demonstration
â”œâ”€â”€ run_tests.py                  # Comprehensive test suite
â””â”€â”€ requirements.txt              # Dependencies
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Create virtual environment
python -m venv ml_env
source ml_env/bin/activate  # Linux/Mac
# or
ml_env\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Run System Demo
```bash
python demo.py
```
*Demonstrates complete navigation pipeline with live camera or test images*

### 3. Run Comprehensive Tests
```bash
python run_tests.py
```
*Validates all system components (expected: 6/6 tests passing)*

### 4. Test Individual Components
```bash
# Test semantic mapping and pathfinding
python test_semantic_mapping.py
python test_pathfinding.py
```

## ğŸ¯ System Capabilities

### **Real-time Navigation Assistance**
- Detects library objects with 85.7% accuracy
- Classifies environments (computer labs, study areas, reading areas)
- Plans optimal paths around obstacles in 30-50ms
- Provides natural language navigation guidance

### **Intelligent Environment Understanding**
- Builds persistent spatial maps of library layouts  
- Tracks object movement and environmental changes
- Learns familiar locations and navigation patterns
- Adapts to dynamic environments in real-time

### **Multi-Algorithm Pathfinding**
- **A***: Optimal paths for stable environments
- **D***: Dynamic replanning for changing conditions
- **RRT***: Complex space exploration and navigation
- **Auto-selection**: Chooses best algorithm for each scenario

## ğŸ“Š Performance Metrics

| Component | Performance | Notes |
|-----------|-------------|--------|
| **YOLO Detection** | 85.7% mAP@0.5 | 6 object classes, real-time capable |
| **A* Pathfinding** | 17-50ms planning | Optimal paths, 170-303 nodes explored |
| **RRT* Pathfinding** | 320ms with optimization | Complex environments, probabilistic |
| **Environment Classification** | >90% accuracy | Computer labs, study areas, etc. |
| **Integration Pipeline** | Real-time capable | Complete detectionâ†’guidance pipeline |

## ğŸ® Usage Examples

### Basic Navigation
```python
from src.pathfinding.navigation_planner import NavigationPlanner, NavigationRequest

# Initialize system
planner = NavigationPlanner(image_width=640, image_height=480)

# Update with camera input
planner.update_environment(yolo_detections, location_hint="Library entrance")

# Plan navigation
request = NavigationRequest(
    goal_description="find computer lab",
    start_pixel_pos=(50.0, 50.0)
)

result = planner.plan_navigation(request)
print(f"Next action: {result.next_action}")
```

### Complete Pipeline
```python
from demo import run_demo
run_demo()  # Full system demonstration
```

## ğŸ§ª Testing & Validation

**Test Suite Results: 6/6 PASSING** âœ…

- **Semantic Mapping**: Environment understanding and memory systems
- **A* Pathfinding**: Optimal path planning validation
- **Algorithm Comparison**: Performance benchmarking
- **Navigation Integration**: End-to-end system testing  
- **Visualization**: Path generation and analysis
- **Live Demo**: Real-world scenario validation

## ğŸ”® Future Enhancements (Sprint 2)

- **ğŸ§ Multimodal Feedback**: Voice guidance + haptic feedback
- **ğŸ“± Mobile Integration**: Smartphone app interface
- **â˜ï¸ Cloud Mapping**: Shared navigation knowledge
- **ğŸ¢ Multi-floor Support**: 3D navigation capabilities
- **ğŸ‘¥ Social Navigation**: Crowd-aware pathfinding

## ğŸ‰ Project Success

**The AI-Assisted Navigation Device is now fully functional and ready for deployment!**

- Complete navigation pipeline from camera input to user guidance
- Production-ready performance with comprehensive error handling
- Extensible architecture for future enhancements
- Validated through extensive testing and real-world scenarios

---

## ğŸ“‹ Detailed Technical Information

### Object Detection Classes
The YOLO model detects 6 library object classes:
- **Books** - Reading materials and collections
- **Monitor** - Computer screens and displays  
- **Office-chair** - Seating furniture
- **Table** - Work and study surfaces
- **Whiteboard** - Presentation and writing surfaces
- **TV** - Television screens and displays

### Model Performance Details
- **Best Model**: YOLOv8s with heavy augmentation
- **Training**: 100-250 epochs on 675 training images
- **Validation**: 80 validation images with comprehensive metrics
- **Real-time Performance**: Capable of processing live camera feeds

### Architecture Components
- **Navigation Pipeline**: Integrates YOLO detection with LLM reasoning
- **Semantic Map Builder**: Creates persistent spatial understanding
- **Scene Memory System**: Tracks temporal object relationships
- **Grid Map Converter**: Transforms detections into navigable representations
- **Multiple Pathfinders**: A*, D*, and RRT* algorithms with auto-selection

*Developed for SIT374 - Capstone Team Project, Deakin University*